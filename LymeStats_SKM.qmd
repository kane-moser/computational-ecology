---
title: "Lyme stats project code update"
author: "Kane Moser"
format: pdf
editor: visual
---

## [Plan]{.underline}

Here are the variables/parameters we want to have:

$X_{t,s}$: Lyme disease annual case incidence per state

$\Delta X_{t,s} = X_{t,s} - X_{t-1,s}$: delta X or change in incidence per state from year to year

$r_{t,s} = \frac{\Delta X_{t,s}}{X_{t-1,s}}$: rate of change in incidence per state from year to year

$\overline{r}_s = \frac{1}{14} \sum{r_{t,s}}$: mean rate of incidence change per state (this will be calculated from 2008-2021 data only)

$\tilde{X}_{t,s} = X_{t,s} \overline{r}_s$: predicted annual incidence based on previous step

$k_s = \frac{X_{t,s} - \tilde{X}_{t,s}}{\tilde{X}_{t,s}}$: difference between actual and predicted incidence (discrepancy presumably caused by change in case def.

Goal is to compare the distribution of $k_s$ for high and low incidence areas to then be able to test some simple hypotheses.

Possible hypotheses to test:

1.  We expect the mean of the distribution of $k_s$ for low incidence states to be centered on 0.
2.  We expect the mean of the distribution of $k_s$ for high incidence states to *not* be centered on 0.
3.  We expect the mean of the distribution of $k_s$ for high and low incidence states to be different from each other.

### 1. Load packages and clean data:

```{r load data}
library(tidyverse)

data <- read_csv("data/Lyme_Disease_Incidence_Rates_by_State_or_Locality.csv")
# Remove special characters from "State" column
data$State <- str_remove_all(data$State, "[^[:alnum:] ]")
# Put data in long format
data <- data %>% pivot_longer(!State, names_to = "Year", values_to = "Incidence")

jurisdiction_data <- read_csv("data/Lyme_jurisdiction_data.csv") %>%
  rename("State" = "states")

data <- data %>%
  left_join(jurisdiction_data, by="State") %>%
  select(State,Year,Incidence,jurisdiction) %>%
  filter(Incidence > 0)
```

### 2. Calculate variable values

```{r}
# calculate delta_X
data <- data %>%
  group_by(State) %>%
  mutate(X = Incidence) %>%
  mutate(delta_X = X - lag(X)) %>%
  ungroup()

# calculate r_t
data <- data %>%
  group_by(State) %>%
  mutate(r = delta_X / lag(X)) %>%
  ungroup()

# calculate mean rate of incidence change per state, mean_r
mean_rates <- data %>%
  filter(Year >= 2008, Year <= 2021) %>% # filter years for 2008-2021
  group_by(State) %>%
  summarise(mean_r = mean(r, na.rm = TRUE))

# join mean rates back to main data
data <- data %>%
  left_join(mean_rates, by = "State")
```

```{r}
# Calculate X_pred and k
# predicted value for 2022, X_pred
state_data <- data %>%
  group_by(State) %>%
  mutate(X_pred = ifelse(Year == 2022, lag(X) * mean_r, NA)) %>%
  ungroup() %>%
  na.omit()

# difference between predicted value and actual value, k
state_data <- state_data %>%
  mutate(k = (X - X_pred) / X_pred) %>%
  filter(!is.infinite(k), !is.na(k))
```

### Compare the distribution of k for low and high incidence states

```{r}
p1 <- ggplot(state_data, aes(x=k, fill = jurisdiction)) +
  geom_histogram(color = "grey40", position="dodge") +
  #facet_wrap(~jurisdiction) +
  theme_minimal() +
  scale_fill_viridis_d()
p1
```

### Hypothesis testing

```{r}
low <- state_data %>% filter(jurisdiction == "low")
high <- state_data %>% filter(jurisdiction == "high")
summary(low)
summary(high)

t.test(low$k, mu=0)
```

```{r}
t.test(high$k, mu = 0)
```

```{r}
t.test(k ~ jurisdiction, state_data)
```

### Uncertainty Quantification - Markov Chain Monte Carlo 

```{r}
low_k <- state_data %>% filter(jurisdiction == "low") %>% pull(k)
high_k <- state_data %>% filter(jurisdiction == "high") %>% pull(k)

# Metropolis-Hastings function
mh_sampler <- function(data, n_iter = 10000, proposal_sd = 0.1) {
  # Initialize parameters
  mu_current <- mean(data)  # Start at the sample mean
  samples <- numeric(n_iter)  # Store samples
  sigma <- sd(data)          # Fixed standard deviation (from data)
  
  # Prior: Normal(0, 10^2)
  prior <- function(mu) {
    dnorm(mu, mean = 0, sd = 10, log = TRUE)
  }
  
  # Likelihood: Normal(mu, sigma^2)
  likelihood <- function(mu) {
    sum(dnorm(data, mean = mu, sd = sigma, log = TRUE))
  }
  
  # Posterior: likelihood * prior
  posterior <- function(mu) {
    likelihood(mu) + prior(mu)  # Log-scale
  }
  
  # MCMC Sampling
  for (i in 1:n_iter) {
    # Propose new mu
    mu_proposed <- rnorm(1, mean = mu_current, sd = proposal_sd)
    
    # Acceptance ratio
    R <- exp(posterior(mu_proposed) - posterior(mu_current))
    
    # Accept or reject
    if (runif(1) < R) {
      mu_current <- mu_proposed
    }
    
    # Store the current sample
    samples[i] <- mu_current
  }
  
  return(samples)
}

# Run the sampler for both groups
low_samples <- mh_sampler(low_k, n_iter = 10000)
high_samples <- mh_sampler(high_k, n_iter = 10000)

# Summarize results
summary(low_samples)
summary(high_samples)

# Compute posterior difference
diff_samples <- high_samples - low_samples
summary(diff_samples)

# Plot results
hist(diff_samples, breaks = 30, main = "Posterior Difference in Means", xlab = "mu_high - mu_low")
abline(v = 0, col = "red", lwd = 2)
```
